class generator_config(object):
    """Wrapper class for generator hyperparameter"""
    
    def __init__(self):
        self.emb_dim = 32  #dimension of embedding
        self.num_emb = 21 #dimension of output unit
        self.hidden_dim = 32 #dimension of hidden unit
        self.sequence_length = 100 #####################################maximum input sequence length
        self.gen_batch_size = 32 #batch size of generator
        self.start_token = 0 #special token for start of sentence

        
class discriminator_config(object):
    """Wrapper class for discriminator hyperparameter"""
    
    def __init__(self):
        self.sequence_length = 100 #######################################maximum input sequence length
        self.num_classes = 2 #############################################number of class (real and fake)
        self.vocab_size = 21 #vocabulary size, shoud be same as num_emb
        self.dis_embedding_dim = 64 #dimension of discriminator embedding space
        self.dis_filter_sizes = [1, 3,5, 10, 15, 20] #convolutional kernel size of discriminator
        self.dis_num_filters = [100, 300, 300, 300,300,100] #number of filters of each conv. kernel
        self.dis_dropout_keep_prob = 0.5 # dropout rate of discriminator
        self.dis_l2_reg_lambda = 0.2 #L2 regularization strength
        self.dis_batch_size = 32 #Batch size for discriminator
        self.dis_learning_rate = 1e-4 #Learning rate of discriminator
        
class training_config(object):
        
    def __init__(self):
        self.gen_learning_rate = 0.005 #learning rate of generator
        self.gen_update_time = 1 #update times of generator in adversarial training
        self.dis_update_time_adv = 5 ##############################################update times of discriminator in adversarial training
        self.dis_update_epoch_adv = 3 #update epoch / times of discriminator
        self.dis_update_time_pre = 80 ##############################################pretraining times of discriminator
        self.dis_update_epoch_pre = 3 #number of epoch / time in pretraining
        self.pretrained_epoch_num = 1 ############################################Number of pretraining epoch
        self.rollout_num = 4 #Rollout number for reward estimation
        self.test_per_epoch = 5 #Test the NLL per epoch
        self.batch_size = 32 #i###############Batch size used for training
        self.save_pretrained = 120 # Whether to save model in certain epoch (optional)
        self.grad_clip = 5.0 #Gradient Clipping 
        self.seed = 88 #Random seed used for initialization
        self.start_token = 0 #special start token
        self.total_batch = 30 ####################################################total batch used for adversarial training
        self.positive_file = "save/Extracellular.txt"  # 1510 save path of real data generated by target LSTM
        self.negative_feedback = "save/gen.txt"  
        self.negative_file = "save/Nucleus.txt" #save path of fake data generated by generator     928
        self.eval_file = "save/Extracellular.txt" #file used for evaluation
        self.generated_num = 1500 #1973Number of samples from generator used for evaluation == self.negative_file
